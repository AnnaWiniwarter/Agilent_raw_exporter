{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4479533-d055-403b-92b9-3fc60556f185",
   "metadata": {},
   "source": [
    "Read Agilent GC raw data, plot and export as csv\n",
    "================================================\n",
    "\n",
    "Using this notebook, you can read in the raw data files as saved by Agilent Chemstation, plot them and export them in `.csv` format. \n",
    "All you need to do it put in the paths to the folders containing the collection of raw data (of the type `my_experiment_name.D`), a label for the legend and colour for the plot, as well as a filename for saving the plot and exporting the data. The comments in this notebook wil guide you through the process.\n",
    "\n",
    "There are two parts: \n",
    "\n",
    "_Part A_ first sets up the libraries and functions required. Then, it allows you to import, plot and export a manual selection of chromatograms based on information you enter (including the raw data filepath, legend label, plotting color and linestyle, as well as filenames for the plot and exported csv file). \n",
    "\n",
    "_Part B_ allows you to quickly export all the runs in all the sequences saved into a shared directory (i.e. a directory containing a directory containing the raw data directories). This automatically selects the name of the run (i.e. whatever is written before `.D`) as the label, makes a plot containing all the runs in a sequence and saves the sequence and the plot automatically using the name of the sequence. \n",
    "\n",
    "The sampling rate of Chemstation is very high (60 Hz). In order to reduce the file size of the exported datafiles to an acceptable level, this was downsampled to 6 Hz. There is a comment in the code highlighting where this is done (in the definition of the function `plot_raw_and_export_as_csv()`, if you would like to deactivate the downsampling. \n",
    "\n",
    "_Important_, as of November 2024, this notebook does not contain any functionality for integrating peaks or reading integration reports or the like.\n",
    "\n",
    "Authors: Anna Winiwarter (anna.winiwarter@radicaldot.com)\n",
    "based on previous work by Kenneth Nielsen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382352f9-a559-4d4e-b27a-00b14ec69164",
   "metadata": {},
   "source": [
    "Part A\n",
    "------\n",
    "\n",
    "Let's start by importing the necessary external libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad25a04a-a68a-43a2-ab04-00660abb24b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import struct\n",
    "import numpy\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.colors as mcolors\n",
    "from pathlib import Path\n",
    "from struct import unpack\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacac1ee-5d26-4146-8644-90a33aff59cc",
   "metadata": {},
   "source": [
    "STEP 1\n",
    "\n",
    "Specify the folder containing the \".D\" files. Note, if you would like to co-plot files from different folders, \n",
    "you can add the folder path directly in front of the file name in step 2.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f698292-3296-4d90-85a4-88f47430106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_folder = Path(r\"Raw data\\2024-10-31-my-experiment_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f489eedd-5f3e-4fc8-b835-2c24a83848eb",
   "metadata": {},
   "source": [
    "STEP 2\n",
    "\n",
    "Specify the paths to the files, a label, color and linestyle. You can add as many files as you would like to the list. If you need to export a lot of files it might be easier to use the more automated approach in Part B.\n",
    "\n",
    "For valid color names in Python check out this page: https://matplotlib.org/stable/gallery/color/named_colors.html\n",
    "\n",
    "For linestyles, see here: https://matplotlib.org/stable/gallery/lines_bars_and_markers/linestyles.html \n",
    "\n",
    "\"-\" is solid, \":\" is dotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300f2b86-f784-451d-b9ab-b92320ab931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths_Ch1 = [\n",
    "    # (r'xyz.D', legend name, 'blue', \"-\"), \n",
    "              (r'SAMPLE_1.D', 'label 1', 'lightblue', \"-\"), # enter your own file, label color and linestyle here. \n",
    "              ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeb8581-9270-4624-b6e4-ce95b3bc13b9",
   "metadata": {},
   "source": [
    "STEP 3\n",
    "\n",
    "Specify a name for saving the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66de5ed-ff74-44fe-b40c-ecee24380b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_name = \"my_test_plot_01\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b3671e-5424-4f9c-abea-338a9455af2c",
   "metadata": {},
   "source": [
    "STEP 4\n",
    "\n",
    "Specify a name for the exported csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f623e2-40c8-45a7-9c09-3aed2403ee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_name = \"my_test_file_01\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96720798-02ce-4071-b24e-02a8a5e34624",
   "metadata": {},
   "source": [
    "STEP 5: \n",
    "\n",
    "Run all the code below.\n",
    "_Important_: You need to run the big chunks of code below, even if you want to use Part B only, as the later part uses some of the functions defined here.\n",
    "\n",
    "First, we need to run some code which will allow us to read the raw binary data files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8ed03b-604f-4a39-b986-e58a32bf381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Classes and methods for reading .ch file ---------------\n",
    "# This part of the code is written by Kenneth Nielsen.\n",
    "# See https://github.com/CINF/PyExpLabSys/blob/master/PyExpLabSys/file_parsers/chemstation.py\n",
    "# The class/methods used in this script is copied here for the convenience of the user. For the full parser, check the original file.\n",
    "\n",
    "# Constants used for binary file parsing\n",
    "ENDIAN = '>'\n",
    "STRING = ENDIAN + '{}s'\n",
    "UINT8 = ENDIAN + 'B'\n",
    "UINT16 = ENDIAN + 'H'\n",
    "INT16 = ENDIAN + 'h'\n",
    "INT32 = ENDIAN + 'i'\n",
    "\n",
    "def parse_utf16_string(file_, encoding='UTF16'):\n",
    "    \"\"\"Parse a pascal type UTF16 encoded string from a binary file object\"\"\"\n",
    "    # First read the expected number of CHARACTERS\n",
    "    string_length = unpack(UINT8, file_.read(1))[0]\n",
    "    # Then read and decode\n",
    "    parsed = unpack(STRING.format(2 * string_length),\n",
    "                    file_.read(2 * string_length))\n",
    "    return parsed[0].decode(encoding)\n",
    "\n",
    "\n",
    "class CHFile(object):\n",
    "    \"\"\"Class that implementats the Agilent .ch file format version 179\n",
    "\n",
    "    .. warning:: Not all aspects of the file header is understood, so there may and probably\n",
    "       is information that is not parsed. See the method :meth:`._parse_header_status` for\n",
    "       an overview of which parts of the header is understood.\n",
    "\n",
    "    .. note:: Although the fundamental storage of the actual data has change, lots of\n",
    "       inspiration for the parsing of the header has been drawn from the parser in the\n",
    "       `ImportAgilent.m file <https://github.com/chemplexity/chromatography/blob/dev/\n",
    "       Methods/Import/ImportAgilent.m>`_ in the `chemplexity/chromatography project\n",
    "       <https://github.com/chemplexity/chromatography>`_ project. All credit for the parts\n",
    "       of the header parsing that could be reused goes to the author of that project.\n",
    "\n",
    "    Attributes:\n",
    "        values (numpy.array): The internsity values (y-value) or the spectrum. The unit\n",
    "            for the values is given in `metadata['units']`\n",
    "        metadata (dict): The extracted metadata\n",
    "        filepath (str): The filepath this object was loaded from\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Fields is a table of name, offset and type. Types 'x-time' and 'utf16' are specially\n",
    "    # handled, the rest are format arguments for struct unpack\n",
    "    fields = (\n",
    "        ('sequence_line_or_injection', 252, UINT16),\n",
    "        ('injection_or_sequence_line', 256, UINT16),\n",
    "        ('start_time', 282, 'x-time'),\n",
    "        ('end_time', 286, 'x-time'),\n",
    "        ('version_string', 326, 'utf16'),\n",
    "        ('description', 347, 'utf16'),\n",
    "        ('sample', 858, 'utf16'),\n",
    "        ('operator', 1880, 'utf16'),\n",
    "        ('date', 2391, 'utf16'),\n",
    "        ('inlet', 2492, 'utf16'),\n",
    "        ('instrument', 2533, 'utf16'),\n",
    "        ('method', 2574, 'utf16'),\n",
    "        ('software version', 3601, 'utf16'),\n",
    "        ('software name', 3089, 'utf16'),\n",
    "        ('software revision', 3802, 'utf16'),\n",
    "        ('units', 4172, 'utf16'),\n",
    "        ('detector', 4213, 'utf16'),\n",
    "        ('yscaling', 4732, ENDIAN + 'd')\n",
    "    )\n",
    "    # The start position of the data\n",
    "    data_start = 6144\n",
    "    # The versions of the file format supported by this implementation\n",
    "    supported_versions = {179} #should be 179\n",
    "\n",
    "    def __init__(self, filepath):\n",
    "        \"\"\"Instantiate object\n",
    "\n",
    "        Args:\n",
    "            filepath (str): The path of the data file\n",
    "        \"\"\"\n",
    "        self.filepath = filepath\n",
    "        self.metadata = {}\n",
    "        with open(self.filepath, 'rb') as file_:\n",
    "            self._parse_header(file_)\n",
    "            self.values = self._parse_data(file_)\n",
    "\n",
    "    def _parse_header(self, file_):\n",
    "        \"\"\"Parse the header\"\"\"\n",
    "        # Parse and check version\n",
    "        length = unpack(UINT8, file_.read(1))[0]\n",
    "        parsed = unpack(STRING.format(length), file_.read(length))\n",
    "        version = int(parsed[0])\n",
    "        if version not in self.supported_versions:\n",
    "            raise ValueError('Unsupported file version {}'.format(version))\n",
    "        self.metadata['magic_number_version'] = version\n",
    "\n",
    "        # Parse all metadata fields\n",
    "        for name, offset, type_ in self.fields:\n",
    "            file_.seek(offset)\n",
    "            if type_ == 'utf16':\n",
    "                self.metadata[name] = parse_utf16_string(file_)\n",
    "            elif type_ == 'x-time':\n",
    "                self.metadata[name] = unpack(ENDIAN + 'f', file_.read(4))[0] / 60000\n",
    "            else:\n",
    "                self.metadata[name] = unpack(type_, file_.read(struct.calcsize(type_)))[0]\n",
    "\n",
    "        # Convert date\n",
    "        self.metadata['datetime'] = time.strptime(self.metadata['date'], '%d-%b-%y, %H:%M:%S')\n",
    "\n",
    "    def _parse_header_status(self):\n",
    "        \"\"\"Print known and unknown parts of the header\"\"\"\n",
    "        file_ = open(self.filepath, 'rb')\n",
    "        # Map positions to fields for all the known fields\n",
    "        knowns = {item[1]: item for item in self.fields}\n",
    "        # A couple of places has a \\x01 byte before a string, these we simply skip\n",
    "        skips = {325, 3600}\n",
    "        # Jump to after the magic number version\n",
    "        file_.seek(4)\n",
    "\n",
    "        # Initialize variables for unknown bytes\n",
    "        unknown_start = None\n",
    "        unknown_bytes = b''\n",
    "        # While we have not yet reached the data\n",
    "        while file_.tell() < self.data_start:\n",
    "            current_position = file_.tell()\n",
    "            # Just continue on skip bytes\n",
    "            if current_position in skips:\n",
    "                file_.read(1)\n",
    "                continue\n",
    "\n",
    "            # If we know about a data field that starts at this point\n",
    "            if current_position in knowns:\n",
    "                # If we have collected unknown bytes, print them out and reset\n",
    "                if unknown_bytes != b'':\n",
    "                    print('Unknown at', unknown_start, repr(unknown_bytes.rstrip(b'\\x00')))\n",
    "                    unknown_bytes = b''\n",
    "                    unknown_start = None\n",
    "\n",
    "                # Print out the position, type, name and value of the known value\n",
    "                print('Known field at {: >4},'.format(current_position), end=' ')\n",
    "                name, _, type_ = knowns[current_position]\n",
    "                if type_ == 'x-time':\n",
    "                    print('x-time, \"{: <19}'.format(name + '\"'),\n",
    "                          unpack(ENDIAN + 'f', file_.read(4))[0] / 60000)\n",
    "                elif type_ == 'utf16':\n",
    "                    print(' utf16, \"{: <19}'.format(name + '\"'),\n",
    "                          parse_utf16_string(file_))\n",
    "                else:\n",
    "                    size = struct.calcsize(type_)\n",
    "                    print('{: >6}, \"{: <19}'.format(type_, name + '\"'),\n",
    "                          unpack(type_, file_.read(size))[0])\n",
    "            else:  # We do not know about a data field at this position If we have already\n",
    "                # collected 4 zero bytes, assume that we are done with this unkonw field,\n",
    "                # print and reset\n",
    "                if unknown_bytes[-4:] == b'\\x00\\x00\\x00\\x00':\n",
    "                    print('Unknown at', unknown_start, repr(unknown_bytes.rstrip(b'\\x00')))\n",
    "                    unknown_bytes = b''\n",
    "                    unknown_start = None\n",
    "\n",
    "                # Read one byte and save it\n",
    "                one_byte = file_.read(1)\n",
    "                if unknown_bytes == b'':\n",
    "                    # Only start a new collection of unknown bytes, if this byte is not a\n",
    "                    # zero byte\n",
    "                    if one_byte != b'\\x00':\n",
    "                        unknown_bytes = one_byte\n",
    "                        unknown_start = file_.tell() - 1\n",
    "                else:\n",
    "                    unknown_bytes += one_byte\n",
    "\n",
    "        file_.close()\n",
    "\n",
    "    def _parse_data(self, file_):\n",
    "        \"\"\"Parse the data\"\"\"\n",
    "        # Go to the end of the file and calculate how many points 8 byte floats there are\n",
    "        file_.seek(0, 2)\n",
    "        n_points = (file_.tell() - self.data_start) // 8\n",
    "\n",
    "        # Read the data into a numpy array\n",
    "        file_.seek(self.data_start)\n",
    "        return numpy.fromfile(file_, dtype='<d', count=n_points) * self.metadata['yscaling']\n",
    "\n",
    "    def times(self):\n",
    "        \"\"\"The time values (x-value) for the data set in minutes\"\"\"\n",
    "        return numpy.linspace(self.metadata['start_time'], self.metadata['end_time'],\n",
    "                              len(self.values))\n",
    "\n",
    "# ----------- Function for plotting and exporting as csv file ---------------\n",
    "def plot_raw_and_export_as_csv(file_paths, file_folder, plot_name, csv_name, axis_y_lims=(-1, 500), downsampling_rate=10):\n",
    "    # Create a figure and a grid of subplots\n",
    "    fig, ax = plt.subplots(1, figsize=(9, 6))\n",
    "\n",
    "    # Loop over the file paths for Ch1\n",
    "    data_dict = {} # prepare an empty dictionary to store the data for saving\n",
    "    for file_path, label, color, linestyle in file_paths:\n",
    "        if not \".D\" in file_path:\n",
    "            continue\n",
    "        full_path = file_folder/file_path/\"FID1A.ch\"\n",
    "        try:\n",
    "            # Read the file using the .ch class defined above.\n",
    "            ch1 = CHFile(full_path)\n",
    "        except FileNotFoundError:\n",
    "            print(\"Could not find a file at: \" + str(full_path))\n",
    "            print(\"Moving on...\")\n",
    "            continue\n",
    "        # Get the y values from the CHFile class and DOWNSAMPLE\n",
    "        ch1_y_raw = ch1.values\n",
    "        ch1_y = signal.decimate(ch1_y_raw, downsampling_rate)\n",
    "        data_dict[label+\"_values\"] = ch1_y\n",
    "    \n",
    "        # Get the x values using the method \"times\" to generate the time value in min from the metadata (start time and end time)\n",
    "        ch1_x_raw = ch1.times()\n",
    "        ch1_x = signal.decimate(ch1_x_raw, downsampling_rate)\n",
    "        # Add to the dictionary\n",
    "        data_dict[label+\"_times\"] = ch1_x\n",
    "    \n",
    "        # Plot the raw chromatogram with the filenname as label and color and linestyle as selected.\n",
    "        ax.plot(ch1_x, ch1_y, label=label, color=color, linestyle=linestyle)\n",
    "   \n",
    "    ax.set_ylabel('Raw signal / pA')\n",
    "    ax.set_xlabel('Time / min')\n",
    "    ax.legend()\n",
    "    ax.set_xlim([-1, 29.5])\n",
    "    ax.set_ylim(axis_y_lims) # adjust the y-axis scale depending on the height of your peaks of interest.\n",
    "    \n",
    "    # Adjust the layout so that there is enough space between the subplots\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_name + \".png\", dpi=600)\n",
    "    plt.show()\n",
    "    \n",
    "    # Export the data as csv\n",
    "    d_sorted = {k: v for k, v in sorted(data_dict.items())}\n",
    "    export_df = pd.DataFrame(\n",
    "                        {key: pd.Series(value) for key, value in d_sorted.items()}\n",
    "                    )\n",
    "    export_df.to_csv(csv_name + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c3f8f6-a4a9-4755-a1f9-8c42e5124c27",
   "metadata": {},
   "source": [
    "Now we can load the raw data, plot it and export it as csv. This function will use the file path information, plot name etc as you entered above in steps 1-4. \n",
    "\n",
    "Seeing that the autoscale of the y-axis will often not be very meaningful, you can set different limits to the y-axis here. Also, if you would prefer to export the dataset at the original sampling rate of 60Hz, set the downsampling_rate to 1. Note, that this is not recommended, as the resulting csv files become very large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ab474-7f67-484f-b4cc-516ae538857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_raw_and_export_as_csv(file_paths_Ch1, file_folder, plot_name, csv_name, axis_y_lims=(-1, 500), downsampling_rate=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f020bd-8571-4302-ad92-18e13396ec9e",
   "metadata": {},
   "source": [
    "Part B\n",
    "------\n",
    "\n",
    "If you'd like to export the data from a number of folders, you can also rely on this semi automatic approach below.\n",
    "\n",
    "First, define the folder path in which you expect the files.\n",
    "\n",
    "Agilent Chemstation saves the data recorded within a sequence (i.e. one set of samples that were run in one go) in a directory, which then contains the raw data for the individual runs (or samples) in subdirectories with the ending `.D`. The `folder_path`asked for here is the root directory containing the sequence directories. I.e if the full path to the raw data file is `\"My raw data/my favourite sequence/sample_01.D/FID1.ch\"` then what you want to enter here is `\"My raw data\"`. (the `r`in front of a string just means that it will read it as raw string and tolerate spaces and other non-standard characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbcd023-6425-426c-8b51-b6e3635baee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = Path(r\"Raw data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2435910b-3f75-40c9-a6f8-722f7fe7b1e6",
   "metadata": {},
   "source": [
    "The cell below will then find all the relevant files in the sub-directories of the path given above and print them (to double check, this should now contain all the `.D` files you'd like to export)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f3d7e6-02d6-4cd6-b38c-8d502fbaeab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "subfolder_list = []\n",
    "for root, subfolders, files in os.walk(folder_path):\n",
    "    #this is bit annoying to work with, so lets better create a dictionary that has the initial subfolders as key and a list of\n",
    "    #the files as values. then we can iterate through that and that should work if the root is a folder containing folders\n",
    "    #that contain the data)\n",
    "    subfolder_list.append(subfolders)\n",
    "folders_dict = {}\n",
    "for folder in subfolder_list[0]:\n",
    "    for root, subfolders, files in os.walk(os.path.join(folder_path, folder)):\n",
    "        folders_dict[folder] = subfolders\n",
    "        break #this is to not enter into the subfolders but actually add the files in the original folder!!\n",
    "print(folders_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835bd3ea-eda1-44df-a42e-681535a14e0b",
   "metadata": {},
   "source": [
    "Optional: if you don't want to re-plot and re-export all the files every time you add a new measurment folder to your main folder, you can run the following cell, which will check if a csv file exists for a measurement folder, and if this is the case, it will delete the folder from the dictionary which is passed to the loop processing the plotting and exporting. If you'd like to re-export (e.g. because you've made some changes somewhere), simply skip running this cell. \n",
    "\n",
    "If it returns {} as the last line (i.e. and empty dictionary, this means that there is no new data in the directory and therefore, nothing will be plotted/exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd4f576-70e3-4e0c-afa3-f07265614ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders_dict_cut = folders_dict.copy()\n",
    "for series in folders_dict.keys():\n",
    "    if os.path.isfile(str(folder_path) + \"/\" + series + \".csv\"):\n",
    "        print(series + \" has been exported, deleting from the folders_dict.\")\n",
    "        del folders_dict_cut[series]\n",
    "folders_dict = folders_dict_cut\n",
    "print(folders_dict)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076d9bed-8bdf-4b08-b56c-fbee5042861a",
   "metadata": {},
   "source": [
    "Now, let's loop though all the measurment series in the folders dictionary, plot and export them. It's using the same function as in Part A, just feeding them automatically. Like in Part A, also here you can change the settings for the axis limits and the sampling rate (absolutely not recommended here, since the files will become really, really large)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e374a211-33ee-4018-aaee-9f15253c72bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for series in folders_dict.keys():\n",
    "    file_paths_auto = []\n",
    "    for folder, color in zip(folders_dict[series], mcolors.TABLEAU_COLORS.keys()):\n",
    "        if not \".D\" in folder:\n",
    "            continue\n",
    "        file_paths_auto.append((folder, folder, color, \"-\"))\n",
    "    plot_raw_and_export_as_csv(file_paths=file_paths_auto, file_folder=Path(r\"Raw Data/\" + series), plot_name=str(series), csv_name=str(series),\n",
    "                               axis_y_lims=(-1, 500), downsampling_rate=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2255fb4-1aa1-4bf8-b13e-d80e6c451aea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
